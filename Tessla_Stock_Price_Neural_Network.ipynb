{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tessla_Stock_Price_Neural_Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5LIugkPJcrV"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import time\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3UyYB85uzkD"
      },
      "source": [
        "def partition(X, T, n_folds, random_shuffle=True):\n",
        "    np.random.seed(42)\n",
        "    rows = np.arange(X.shape[0])\n",
        "    np.random.shuffle(rows)\n",
        "    X = X[rows, :]\n",
        "    T = T[rows, :]\n",
        "    \n",
        "    n_samples = X.shape[0]\n",
        "    n_per_fold = n_samples // n_folds\n",
        "    n_last_fold = n_samples - n_per_fold * (n_folds - 1)  \n",
        "\n",
        "    folds = []\n",
        "    start = 0\n",
        "    for foldi in range(n_folds-1):\n",
        "        folds.append( (X[start:start + n_per_fold, :], T[start:start + n_per_fold, :]) )\n",
        "        start += n_per_fold\n",
        "    folds.append( (X[start:, :], T[start:, :]) )\n",
        "    len(folds), len(folds[0]), folds[0][0].shape, folds[0][1].shape\n",
        "    Xvalidate, Tvalidate = folds[0]\n",
        "    Xtest, Ttest = folds[1]\n",
        "    Xtrain, Ttrain = np.vstack([X for (X, _) in folds[2:]]), np.vstack([T for (_, T) in folds[2:]])\n",
        "    Xtrain.shape, Ttrain.shape, Xvalidate.shape, Tvalidate.shape, Xtest.shape, Ttest.shape\n",
        "\n",
        "    return Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5hQKfKuxxWJ"
      },
      "source": [
        "def confusion_matrix(Y_classes, T):\n",
        "  class_names=np.unique(T)\n",
        "  table = []\n",
        "  for true_class in class_names:\n",
        "    row = []\n",
        "    for predicted_class in class_names:\n",
        "        row.append(100 * np.mean(Y_classes[T == true_class] == predicted_class))\n",
        "    table.append(row)\n",
        "    \n",
        "  print(f'Test percent correct {np.mean(Y_classes == T) * 100:.2f}') \n",
        "\n",
        "  return pandas.DataFrame(table, index=class_names, columns=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7scGwyUKr3a",
        "outputId": "4d0e277b-82c0-416a-a44d-7f01afa68720"
      },
      "source": [
        "%%writefile optimizers.py\n",
        "import numpy as np\n",
        "\n",
        "class Optimizers():\n",
        "\n",
        "    def __init__(self, all_weights):\n",
        "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
        "        \n",
        "        self.all_weights = all_weights\n",
        "\n",
        "        self.mt = np.zeros_like(all_weights)\n",
        "        self.vt = np.zeros_like(all_weights)\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.beta1t = 1\n",
        "        self.beta2t = 1\n",
        "\n",
        "        \n",
        "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
        "\n",
        "        error_trace = []\n",
        "        epochs_per_print = n_epochs // 10\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            error = error_f(*fargs)\n",
        "            grad = gradient_f(*fargs)\n",
        "\n",
        "            # Update all weights using -= to modify their values in-place.\n",
        "            self.all_weights -= learning_rate * grad\n",
        "\n",
        "            if error_convert_f:\n",
        "                error = error_convert_f(error)\n",
        "            error_trace.append(error)\n",
        "\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
        "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
        "\n",
        "        return error_trace\n",
        "\n",
        "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
        "\n",
        "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
        "        epsilon = 1e-8\n",
        "        error_trace = []\n",
        "        epochs_per_print = n_epochs // 10\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            error = error_f(*fargs)\n",
        "            grad = gradient_f(*fargs)\n",
        "\n",
        "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
        "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
        "            self.beta1t *= self.beta1\n",
        "            self.beta2t *= self.beta2\n",
        "\n",
        "            m_hat = self.mt / (1 - self.beta1t)\n",
        "            v_hat = self.vt / (1 - self.beta2t)\n",
        "\n",
        "            # Update all weights using -= to modify their values in-place.\n",
        "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "    \n",
        "            if error_convert_f:\n",
        "                error = error_convert_f(error)\n",
        "            error_trace.append(error)\n",
        "\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
        "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
        "\n",
        "        return error_trace\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.ion()\n",
        "\n",
        "    def parabola(wmin):\n",
        "        return ((w - wmin) ** 2)[0]\n",
        "\n",
        "    def parabola_gradient(wmin):\n",
        "        return 2 * (w - wmin)\n",
        "\n",
        "    w = np.array([0.0])\n",
        "    optimizer = Optimizers(w)\n",
        "\n",
        "    wmin = 5\n",
        "    optimizer.sgd(parabola, parabola_gradient, [wmin],\n",
        "                  n_epochs=500, learning_rate=0.1)\n",
        "\n",
        "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\n",
        "\n",
        "    w = np.array([0.0])\n",
        "    optimizer = Optimizers(w)\n",
        "    optimizer.adam(parabola, parabola_gradient, [wmin],\n",
        "                   n_epochs=500, learning_rate=0.1)\n",
        "    \n",
        "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting optimizers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhzckJt_pxGr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Aj5K2DJbC_"
      },
      "source": [
        "import optimizers\n",
        "import sys  \n",
        "\n",
        "class NeuralNetwork():\n",
        "\n",
        "\n",
        "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
        "            self.n_hiddens_per_layer = []\n",
        "        else:\n",
        "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
        "\n",
        "        n_in = n_inputs\n",
        "        shapes = []\n",
        "        for nh in self.n_hiddens_per_layer:\n",
        "            shapes.append((n_in + 1, nh))\n",
        "            n_in = nh\n",
        "        shapes.append((n_in + 1, n_outputs))\n",
        "\n",
        "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
        "\n",
        "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
        "\n",
        "        self.trained = False\n",
        "        self.total_epochs = 0\n",
        "        self.error_trace = []\n",
        "        self.Xmeans = None\n",
        "        self.Xstds = None\n",
        "        self.Tmeans = None\n",
        "        self.Tstds = None\n",
        "\n",
        "\n",
        "    def make_weights_and_views(self, shapes):\n",
        "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
        "                                 for shape in shapes])\n",
        "        views = []\n",
        "        start = 0\n",
        "        for shape in shapes:\n",
        "            size =shape[0] * shape[1]\n",
        "            views.append(all_weights[start:start + size].reshape(shape))\n",
        "            start += size\n",
        "        return all_weights, views\n",
        "\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
        "\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        result = self.__repr__()\n",
        "        if len(self.error_trace) > 0:\n",
        "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
        "    def makeIndicatorVars(self, T):\n",
        "          if T.ndim == 1:\n",
        "              T = T.reshape((-1, 1))\n",
        "          retT = (T == np.unique(T)).astype(int)\n",
        "          return retT\n",
        "          \n",
        "    def softmax (self, X):\n",
        "        fs = np.exp(X)  \n",
        "        denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
        "        gs = fs / (denom + sys.float_info.epsilon)\n",
        "        return gs\n",
        "\n",
        "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
        "\n",
        "        if self.Xmeans is None:\n",
        "            self.Xmeans = X.mean(axis=0)\n",
        "            self.Xstds = X.std(axis=0)\n",
        "            self.Xstds[self.Xstds == 0] = 1  \n",
        "            self.Tmeans = T.mean(axis=0)\n",
        "            self.Tstds = T.std(axis=0)\n",
        "            \n",
        "        X = (X - self.Xmeans) / self.Xstds\n",
        "        self.TtrainI = self.makeIndicatorVars(T)\n",
        "        self.uniqueT=np.unique(T)\n",
        "        optimizer = optimizers.Optimizers(self.all_weights)\n",
        "\n",
        "\n",
        "        error_convert_f = lambda nll: (np.exp(-nll))\n",
        "\n",
        "        if method == 'sgd':\n",
        "\n",
        "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
        "                                        fargs=[X, self.TtrainI], n_epochs=n_epochs,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        verbose=True,\n",
        "                                        error_convert_f=error_convert_f)\n",
        "\n",
        "        elif method == 'adam':\n",
        "\n",
        "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
        "                                         fargs=[X, self.TtrainI], n_epochs=n_epochs,\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         verbose=True,\n",
        "                                         error_convert_f=error_convert_f)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
        "        \n",
        "        self.error_trace = error_trace\n",
        "\n",
        "        return self\n",
        "\n",
        "    def relu(self, s):\n",
        "        s[s < 0] = 0\n",
        "        return s\n",
        "\n",
        "    def grad_relu(self, s):\n",
        "        return (s > 0).astype(int)\n",
        "    \n",
        "    def forward_pass(self, X):\n",
        "        self.Ys = [X]\n",
        "        for W in self.Ws[:-1]:\n",
        "            if self.activation_function == 'relu':\n",
        "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "            else:\n",
        "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "        last_W = self.Ws[-1]\n",
        "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
        "        return self.Ys\n",
        "\n",
        "    def error_f(self, X, T):\n",
        "        temp = self.forward_pass(X)\n",
        "        Y = self.softmax(temp[-1])\n",
        "        return - np.mean((T * np.log(Y)))\n",
        "\n",
        "    def gradient_f(self, X, T):\n",
        "        Y = self.softmax(self.Ys[-1])\n",
        "        delta = (Y - T) / (T.shape[0] * T.shape[1])\n",
        "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
        "        for layeri in range(n_layers - 1, -1, -1):\n",
        "              self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
        "              self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
        "              if self.activation_function == 'relu':\n",
        "                  delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
        "              else:\n",
        "                  delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
        "        return self.all_gradients\n",
        "\n",
        "    def use(self, X):\n",
        "        Xtest = ((X - self.Xmeans)/self.Xstds)\n",
        "        temp = self.forward_pass(Xtest)\n",
        "        logregOutput = self.softmax(temp[-1])\n",
        "        predictedTrain = np.argmax(logregOutput,axis=1)\n",
        "        result = np.array(list(map(lambda x: [self.uniqueT[x].astype(int)] , predictedTrain)))\n",
        "        return result , logregOutput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "TCcFSPOs6Sjv",
        "outputId": "6d5b0a25-d23b-4bd8-f221-06f5fbe329b9"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9139cde4-c459-4031-b18c-7621e9b3a8b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9139cde4-c459-4031-b18c-7621e9b3a8b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Team_Project_Final_Dataset.csv to Team_Project_Final_Dataset (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyhyZTdw6Wet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "ae933618-3dfd-4924-f609-b812fb664920"
      },
      "source": [
        "FileData = pandas.read_csv('Team_Project_Final_Dataset.csv', delimiter=',',\n",
        "                          names=('Date', '# of Tweets', '# of +Tweets','# of -Tweets',\n",
        "                                 '# of +Articles', '# of -Articles', '% Diff Dow Jones',\n",
        "                                 '% Diff Tesla'))\n",
        "FileData = FileData.drop([0])\n",
        "FileData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th># of Tweets</th>\n",
              "      <th># of +Tweets</th>\n",
              "      <th># of -Tweets</th>\n",
              "      <th># of +Articles</th>\n",
              "      <th># of -Articles</th>\n",
              "      <th>% Diff Dow Jones</th>\n",
              "      <th>% Diff Tesla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6/28/2013</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6/29/2013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6/30/2013</td>\n",
              "      <td>23.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7/15/2013</td>\n",
              "      <td>525.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>620.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7/15/2013</td>\n",
              "      <td>46.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4198</th>\n",
              "      <td>10/14/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010974</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4199</th>\n",
              "      <td>10/15/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4200</th>\n",
              "      <td>10/16/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4201</th>\n",
              "      <td>3/22/2012</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>8/31/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007059</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4202 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  # of Tweets  ...  % Diff Dow Jones  % Diff Tesla\n",
              "1      6/28/2013        109.0  ...          0.000000             1\n",
              "2      6/29/2013          NaN  ...         -0.002772             1\n",
              "3      6/30/2013         23.0  ...         -0.002772             1\n",
              "4      7/15/2013        525.0  ...          0.001524             7\n",
              "5      7/15/2013         46.0  ...          0.001524             7\n",
              "...          ...          ...  ...               ...           ...\n",
              "4198  10/14/2011          NaN  ...          0.010974             5\n",
              "4199  10/15/2011          NaN  ...         -0.007141             5\n",
              "4200  10/16/2011          NaN  ...         -0.007141             5\n",
              "4201   3/22/2012          3.0  ...         -0.009464             5\n",
              "4202   8/31/2011          NaN  ...          0.007059             5\n",
              "\n",
              "[4202 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "qR728WGl_9vx",
        "outputId": "23ebcb00-ef24-4527-cf17-8c064c20e78e"
      },
      "source": [
        "#FileData = FileData.dropna()\n",
        "FileData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th># of Tweets</th>\n",
              "      <th># of +Tweets</th>\n",
              "      <th># of -Tweets</th>\n",
              "      <th># of +Articles</th>\n",
              "      <th># of -Articles</th>\n",
              "      <th>% Diff Dow Jones</th>\n",
              "      <th>% Diff Tesla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6/28/2013</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6/29/2013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6/30/2013</td>\n",
              "      <td>23.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7/15/2013</td>\n",
              "      <td>525.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>620.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7/15/2013</td>\n",
              "      <td>46.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4198</th>\n",
              "      <td>10/14/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010974</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4199</th>\n",
              "      <td>10/15/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4200</th>\n",
              "      <td>10/16/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4201</th>\n",
              "      <td>3/22/2012</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>8/31/2011</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007059</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4202 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  # of Tweets  ...  % Diff Dow Jones  % Diff Tesla\n",
              "1      6/28/2013        109.0  ...          0.000000             0\n",
              "2      6/29/2013          NaN  ...         -0.002772             0\n",
              "3      6/30/2013         23.0  ...         -0.002772             0\n",
              "4      7/15/2013        525.0  ...          0.001524             4\n",
              "5      7/15/2013         46.0  ...          0.001524             4\n",
              "...          ...          ...  ...               ...           ...\n",
              "4198  10/14/2011          NaN  ...          0.010974             4\n",
              "4199  10/15/2011          NaN  ...         -0.007141             4\n",
              "4200  10/16/2011          NaN  ...         -0.007141             4\n",
              "4201   3/22/2012          3.0  ...         -0.009464             4\n",
              "4202   8/31/2011          NaN  ...          0.007059             4\n",
              "\n",
              "[4202 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYjmQxo5C4EE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "632f82fa-8a9c-4d1e-b9d3-e63812d8b499"
      },
      "source": [
        "#FileData['# of Tweets'] = FileData['# of Tweets'].astype(float)\n",
        "#FileData['# of +Tweets'] = FileData['# of +Tweets'].astype(float)\n",
        "#FileData['# of -Tweets'] = FileData['# of -Tweets'].astype(float)\n",
        "#FileData['# of +Articles'] = FileData['# of +Articles'].astype(float)\n",
        "#FileData['# of -Articles'] = FileData['# of -Articles'].astype(float)\n",
        "#FileData['% Diff Dow Jones'] = FileData['% Diff Dow Jones'].astype(float)\n",
        "#FileData['% Diff Tesla'] = FileData['% Diff Tesla'].astype(int)\n",
        "Orig_FileData=FileData\n",
        "FileData=FileData.replace({'% Diff Tesla': {1: 0,2: 0,3: 0,5: 4,6: 4,7: 4}})\n",
        "FileData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f23afd7e3f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#FileData['% Diff Dow Jones'] = FileData['% Diff Dow Jones'].astype(float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#FileData['% Diff Tesla'] = FileData['% Diff Tesla'].astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mOrig_FileData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFileData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mFileData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFileData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'% Diff Tesla'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mFileData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'FileData' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PptDFm1E9sbw"
      },
      "source": [
        "dates = FileData.iloc[:,1:-1].to_numpy()\n",
        "X = FileData.iloc[:,1:-1].to_numpy()\n",
        "T = FileData.iloc[:,-1:].to_numpy()\n",
        "Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest = partition(X, T, 10, random_shuffle=True)\n",
        "dates2 = Orig_FileData.iloc[:,1:-1].to_numpy()\n",
        "X2 = Orig_FileData.iloc[:,1:-1].to_numpy()\n",
        "T2 = Orig_FileData.iloc[:,-1:].to_numpy()\n",
        "Xtrain2, Ttrain2, Xvalidate2, Tvalidate2, Xtest2, Ttest2 = partition(X2, T2, 10, random_shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2dpAB3bCBfE",
        "outputId": "06b01a22-75fe-4d1d-a683-99aefb8a81f9"
      },
      "source": [
        "nnets=[]\n",
        "for i in [[10],[100],[10,20,10],[10,100],[100,10],[100,10,100],[100,200,100],[10,100,10],[20,50,100,50,20],[10,50,100,200,100,50,10],[200,100,50,10,50,100,200]]:\n",
        "  nnets.append(NeuralNetwork(Xtrain.shape[1], i, 2))\n",
        "for nnet in nnets:\n",
        "  nnet.train(Xtrain, Ttrain, 5000, .0005, method='adam', verbose=True)\n",
        "  print(nnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam: Epoch 500 Error=0.73109\n",
            "Adam: Epoch 1000 Error=0.75918\n",
            "Adam: Epoch 1500 Error=0.77783\n",
            "Adam: Epoch 2000 Error=0.79077\n",
            "Adam: Epoch 2500 Error=0.80102\n",
            "Adam: Epoch 3000 Error=0.80912\n",
            "Adam: Epoch 3500 Error=0.81737\n",
            "Adam: Epoch 4000 Error=0.82477\n",
            "Adam: Epoch 4500 Error=0.83124\n",
            "Adam: Epoch 5000 Error=0.83840\n",
            "NeuralNetwork(6, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.8384\n",
            "Adam: Epoch 500 Error=0.76161\n",
            "Adam: Epoch 1000 Error=0.79626\n",
            "Adam: Epoch 1500 Error=0.82693\n",
            "Adam: Epoch 2000 Error=0.85433\n",
            "Adam: Epoch 2500 Error=0.87750\n",
            "Adam: Epoch 3000 Error=0.89738\n",
            "Adam: Epoch 3500 Error=0.91640\n",
            "Adam: Epoch 4000 Error=0.93257\n",
            "Adam: Epoch 4500 Error=0.94527\n",
            "Adam: Epoch 5000 Error=0.95502\n",
            "NeuralNetwork(6, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.9550\n",
            "Adam: Epoch 500 Error=0.74758\n",
            "Adam: Epoch 1000 Error=0.79969\n",
            "Adam: Epoch 1500 Error=0.85460\n",
            "Adam: Epoch 2000 Error=0.89731\n",
            "Adam: Epoch 2500 Error=0.93062\n",
            "Adam: Epoch 3000 Error=0.96473\n",
            "Adam: Epoch 3500 Error=0.97804\n",
            "Adam: Epoch 4000 Error=0.98471\n",
            "Adam: Epoch 4500 Error=0.98909\n",
            "Adam: Epoch 5000 Error=0.99246\n",
            "NeuralNetwork(6, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9925\n",
            "Adam: Epoch 500 Error=0.75416\n",
            "Adam: Epoch 1000 Error=0.81209\n",
            "Adam: Epoch 1500 Error=0.87092\n",
            "Adam: Epoch 2000 Error=0.90131\n",
            "Adam: Epoch 2500 Error=0.93433\n",
            "Adam: Epoch 3000 Error=0.96255\n",
            "Adam: Epoch 3500 Error=0.98057\n",
            "Adam: Epoch 4000 Error=0.99005\n",
            "Adam: Epoch 4500 Error=0.99485\n",
            "Adam: Epoch 5000 Error=0.99718\n",
            "NeuralNetwork(6, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9972\n",
            "Adam: Epoch 500 Error=0.77829\n",
            "Adam: Epoch 1000 Error=0.83445\n",
            "Adam: Epoch 1500 Error=0.86717\n",
            "Adam: Epoch 2000 Error=0.89721\n",
            "Adam: Epoch 2500 Error=0.92698\n",
            "Adam: Epoch 3000 Error=0.94527\n",
            "Adam: Epoch 3500 Error=0.95728\n",
            "Adam: Epoch 4000 Error=0.96652\n",
            "Adam: Epoch 4500 Error=0.97312\n",
            "Adam: Epoch 5000 Error=0.97751\n",
            "NeuralNetwork(6, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9775\n",
            "Adam: Epoch 500 Error=0.81926\n",
            "Adam: Epoch 1000 Error=0.93583\n",
            "Adam: Epoch 1500 Error=0.96587\n",
            "Adam: Epoch 2000 Error=0.97864\n",
            "Adam: Epoch 2500 Error=0.98720\n",
            "Adam: Epoch 3000 Error=0.99447\n",
            "Adam: Epoch 3500 Error=0.99758\n",
            "Adam: Epoch 4000 Error=0.99885\n",
            "Adam: Epoch 4500 Error=0.99936\n",
            "Adam: Epoch 5000 Error=0.99961\n",
            "NeuralNetwork(6, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9996\n",
            "Adam: Epoch 500 Error=0.73712\n",
            "Adam: Epoch 1000 Error=0.74135\n",
            "Adam: Epoch 1500 Error=0.74772\n",
            "Adam: Epoch 2000 Error=0.74806\n",
            "Adam: Epoch 2500 Error=0.74815\n",
            "Adam: Epoch 3000 Error=0.74819\n",
            "Adam: Epoch 3500 Error=0.74822\n",
            "Adam: Epoch 4000 Error=0.74823\n",
            "Adam: Epoch 4500 Error=0.74824\n",
            "Adam: Epoch 5000 Error=0.74825\n",
            "NeuralNetwork(6, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7482\n",
            "Adam: Epoch 500 Error=0.74915\n",
            "Adam: Epoch 1000 Error=0.80379\n",
            "Adam: Epoch 1500 Error=0.83048\n",
            "Adam: Epoch 2000 Error=0.84809\n",
            "Adam: Epoch 2500 Error=0.86571\n",
            "Adam: Epoch 3000 Error=0.87379\n",
            "Adam: Epoch 3500 Error=0.88226\n",
            "Adam: Epoch 4000 Error=0.88485\n",
            "Adam: Epoch 4500 Error=0.88602\n",
            "Adam: Epoch 5000 Error=0.88662\n",
            "NeuralNetwork(6, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8866\n",
            "Adam: Epoch 500 Error=0.73056\n",
            "Adam: Epoch 1000 Error=0.74945\n",
            "Adam: Epoch 1500 Error=0.80170\n",
            "Adam: Epoch 2000 Error=0.82775\n",
            "Adam: Epoch 2500 Error=0.83739\n",
            "Adam: Epoch 3000 Error=0.85196\n",
            "Adam: Epoch 3500 Error=0.85856\n",
            "Adam: Epoch 4000 Error=0.86130\n",
            "Adam: Epoch 4500 Error=0.86518\n",
            "Adam: Epoch 5000 Error=0.87483\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.8748\n",
            "Adam: Epoch 500 Error=0.73179\n",
            "Adam: Epoch 1000 Error=0.78975\n",
            "Adam: Epoch 1500 Error=0.84418\n",
            "Adam: Epoch 2000 Error=0.90508\n",
            "Adam: Epoch 2500 Error=0.93948\n",
            "Adam: Epoch 3000 Error=0.94710\n",
            "Adam: Epoch 3500 Error=0.96814\n",
            "Adam: Epoch 4000 Error=0.97683\n",
            "Adam: Epoch 4500 Error=0.98505\n",
            "Adam: Epoch 5000 Error=0.98950\n",
            "NeuralNetwork(6, [10, 50, 100, 200, 100, 50, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9895\n",
            "Adam: Epoch 500 Error=0.70863\n",
            "Adam: Epoch 1000 Error=0.70874\n",
            "Adam: Epoch 1500 Error=0.70869\n",
            "Adam: Epoch 2000 Error=0.70861\n",
            "Adam: Epoch 2500 Error=0.70861\n",
            "Adam: Epoch 3000 Error=0.70861\n",
            "Adam: Epoch 3500 Error=0.70861\n",
            "Adam: Epoch 4000 Error=0.70861\n",
            "Adam: Epoch 4500 Error=0.70861\n",
            "Adam: Epoch 5000 Error=0.70861\n",
            "NeuralNetwork(6, [200, 100, 50, 10, 50, 100, 200], 2, 'tanh') trained for 5000 epochs, final training error 0.7086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igTeDn5eI3Bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded17692-12fe-42aa-da1b-dc4fbd26e129"
      },
      "source": [
        "for nnet in nnets:\n",
        "  print(nnet)\n",
        "  Y_classes, Y_probs = nnet.use(Xtest)\n",
        "  print(\"test\")\n",
        "  confusion_matrix(Y_classes, Ttest)\n",
        "  Y_classes, Y_probs = nnet.use(Xvalidate)\n",
        "  print(\"validate\")\n",
        "  confusion_matrix(Y_classes, Tvalidate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(6, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.8384\n",
            "test\n",
            "Test percent correct 71.43\n",
            "validate\n",
            "Test percent correct 64.29\n",
            "NeuralNetwork(6, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.9550\n",
            "test\n",
            "Test percent correct 60.71\n",
            "validate\n",
            "Test percent correct 67.86\n",
            "NeuralNetwork(6, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9925\n",
            "test\n",
            "Test percent correct 71.43\n",
            "validate\n",
            "Test percent correct 60.71\n",
            "NeuralNetwork(6, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9972\n",
            "test\n",
            "Test percent correct 75.00\n",
            "validate\n",
            "Test percent correct 71.43\n",
            "NeuralNetwork(6, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9775\n",
            "test\n",
            "Test percent correct 71.43\n",
            "validate\n",
            "Test percent correct 67.86\n",
            "NeuralNetwork(6, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9996\n",
            "test\n",
            "Test percent correct 67.86\n",
            "validate\n",
            "Test percent correct 64.29\n",
            "NeuralNetwork(6, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7482\n",
            "test\n",
            "Test percent correct 64.29\n",
            "validate\n",
            "Test percent correct 60.71\n",
            "NeuralNetwork(6, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8866\n",
            "test\n",
            "Test percent correct 64.29\n",
            "validate\n",
            "Test percent correct 67.86\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.8748\n",
            "test\n",
            "Test percent correct 50.00\n",
            "validate\n",
            "Test percent correct 71.43\n",
            "NeuralNetwork(6, [10, 50, 100, 200, 100, 50, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9895\n",
            "test\n",
            "Test percent correct 60.71\n",
            "validate\n",
            "Test percent correct 67.86\n",
            "NeuralNetwork(6, [200, 100, 50, 10, 50, 100, 200], 2, 'tanh') trained for 5000 epochs, final training error 0.7086\n",
            "test\n",
            "Test percent correct 35.71\n",
            "validate\n",
            "Test percent correct 57.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "AeZE1rM9kPUM",
        "outputId": "8e993682-fd64-4fbc-8adf-66215e10ee89"
      },
      "source": [
        "Y_classes, Y_probs = nnets[3].use(Xtest)\n",
        "print(\"test\")\n",
        "confusion_matrix(Y_classes, Ttest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "Test percent correct 75.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>20.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27.777778</td>\n",
              "      <td>72.222222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0          4\n",
              "0  80.000000  20.000000\n",
              "4  27.777778  72.222222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "YL0hM34ZoFvJ",
        "outputId": "72338238-38af-45e0-d240-8b93654d99f0"
      },
      "source": [
        "Y_classes, Y_probs = nnets[10].use(Xtest)\n",
        "confusion_matrix(Y_classes, Ttest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test percent correct 35.71\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    4\n",
              "0  100.0  0.0\n",
              "4  100.0  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8sn-d7Dd1dg",
        "outputId": "b1572365-b717-42d6-b72e-653dd44dc675"
      },
      "source": [
        "nnets2=[]\n",
        "for i in [[10],[100],[10,20,10],[10,100],[100,10],[100,10,100],[100,200,100],[10,100,10],[20,50,100,50,20],[10,50,100,200,100,50,10],[200,100,50,10,50,100,200]]:\n",
        "  nnets2.append(NeuralNetwork(Xtrain2.shape[1], i, 7))\n",
        "for nnet in nnets2:\n",
        "  nnet.train(Xtrain2, Ttrain2, 5000, .0005, method='adam', verbose=True)\n",
        "  print(nnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam: Epoch 500 Error=0.79886\n",
            "Adam: Epoch 1000 Error=0.80704\n",
            "Adam: Epoch 1500 Error=0.81466\n",
            "Adam: Epoch 2000 Error=0.82353\n",
            "Adam: Epoch 2500 Error=0.83172\n",
            "Adam: Epoch 3000 Error=0.83811\n",
            "Adam: Epoch 3500 Error=0.84318\n",
            "Adam: Epoch 4000 Error=0.84779\n",
            "Adam: Epoch 4500 Error=0.85146\n",
            "Adam: Epoch 5000 Error=0.85547\n",
            "NeuralNetwork(6, [10], 7, 'tanh') trained for 5000 epochs, final training error 0.8555\n",
            "Adam: Epoch 500 Error=0.81979\n",
            "Adam: Epoch 1000 Error=0.84946\n",
            "Adam: Epoch 1500 Error=0.87352\n",
            "Adam: Epoch 2000 Error=0.89015\n",
            "Adam: Epoch 2500 Error=0.90236\n",
            "Adam: Epoch 3000 Error=0.91421\n",
            "Adam: Epoch 3500 Error=0.92611\n",
            "Adam: Epoch 4000 Error=0.93676\n",
            "Adam: Epoch 4500 Error=0.94607\n",
            "Adam: Epoch 5000 Error=0.95445\n",
            "NeuralNetwork(6, [100], 7, 'tanh') trained for 5000 epochs, final training error 0.9544\n",
            "Adam: Epoch 500 Error=0.80193\n",
            "Adam: Epoch 1000 Error=0.80812\n",
            "Adam: Epoch 1500 Error=0.81297\n",
            "Adam: Epoch 2000 Error=0.81917\n",
            "Adam: Epoch 2500 Error=0.82541\n",
            "Adam: Epoch 3000 Error=0.83186\n",
            "Adam: Epoch 3500 Error=0.83746\n",
            "Adam: Epoch 4000 Error=0.84792\n",
            "Adam: Epoch 4500 Error=0.85354\n",
            "Adam: Epoch 5000 Error=0.86133\n",
            "NeuralNetwork(6, [10, 20, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8613\n",
            "Adam: Epoch 500 Error=0.81193\n",
            "Adam: Epoch 1000 Error=0.83985\n",
            "Adam: Epoch 1500 Error=0.87907\n",
            "Adam: Epoch 2000 Error=0.90730\n",
            "Adam: Epoch 2500 Error=0.93151\n",
            "Adam: Epoch 3000 Error=0.95182\n",
            "Adam: Epoch 3500 Error=0.96818\n",
            "Adam: Epoch 4000 Error=0.97905\n",
            "Adam: Epoch 4500 Error=0.98656\n",
            "Adam: Epoch 5000 Error=0.99173\n",
            "NeuralNetwork(6, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9917\n",
            "Adam: Epoch 500 Error=0.81393\n",
            "Adam: Epoch 1000 Error=0.82830\n",
            "Adam: Epoch 1500 Error=0.84080\n",
            "Adam: Epoch 2000 Error=0.85258\n",
            "Adam: Epoch 2500 Error=0.85971\n",
            "Adam: Epoch 3000 Error=0.86899\n",
            "Adam: Epoch 3500 Error=0.87286\n",
            "Adam: Epoch 4000 Error=0.87750\n",
            "Adam: Epoch 4500 Error=0.88001\n",
            "Adam: Epoch 5000 Error=0.88214\n",
            "NeuralNetwork(6, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8821\n",
            "Adam: Epoch 500 Error=0.83377\n",
            "Adam: Epoch 1000 Error=0.88086\n",
            "Adam: Epoch 1500 Error=0.91653\n",
            "Adam: Epoch 2000 Error=0.93908\n",
            "Adam: Epoch 2500 Error=0.95805\n",
            "Adam: Epoch 3000 Error=0.97043\n",
            "Adam: Epoch 3500 Error=0.97561\n",
            "Adam: Epoch 4000 Error=0.97878\n",
            "Adam: Epoch 4500 Error=0.98221\n",
            "Adam: Epoch 5000 Error=0.98457\n",
            "NeuralNetwork(6, [100, 10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9846\n",
            "Adam: Epoch 500 Error=0.89144\n",
            "Adam: Epoch 1000 Error=0.96145\n",
            "Adam: Epoch 1500 Error=0.98649\n",
            "Adam: Epoch 2000 Error=0.99460\n",
            "Adam: Epoch 2500 Error=0.99719\n",
            "Adam: Epoch 3000 Error=0.99822\n",
            "Adam: Epoch 3500 Error=0.99880\n",
            "Adam: Epoch 4000 Error=0.99916\n",
            "Adam: Epoch 4500 Error=0.99940\n",
            "Adam: Epoch 5000 Error=0.99959\n",
            "NeuralNetwork(6, [100, 200, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9996\n",
            "Adam: Epoch 500 Error=0.80815\n",
            "Adam: Epoch 1000 Error=0.82229\n",
            "Adam: Epoch 1500 Error=0.84142\n",
            "Adam: Epoch 2000 Error=0.85621\n",
            "Adam: Epoch 2500 Error=0.86798\n",
            "Adam: Epoch 3000 Error=0.87666\n",
            "Adam: Epoch 3500 Error=0.88247\n",
            "Adam: Epoch 4000 Error=0.88782\n",
            "Adam: Epoch 4500 Error=0.89075\n",
            "Adam: Epoch 5000 Error=0.89380\n",
            "NeuralNetwork(6, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8938\n",
            "Adam: Epoch 500 Error=0.81692\n",
            "Adam: Epoch 1000 Error=0.82724\n",
            "Adam: Epoch 1500 Error=0.83141\n",
            "Adam: Epoch 2000 Error=0.83440\n",
            "Adam: Epoch 2500 Error=0.83628\n",
            "Adam: Epoch 3000 Error=0.84184\n",
            "Adam: Epoch 3500 Error=0.84979\n",
            "Adam: Epoch 4000 Error=0.85719\n",
            "Adam: Epoch 4500 Error=0.85959\n",
            "Adam: Epoch 5000 Error=0.86174\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.8617\n",
            "Adam: Epoch 500 Error=0.80354\n",
            "Adam: Epoch 1000 Error=0.81018\n",
            "Adam: Epoch 1500 Error=0.81531\n",
            "Adam: Epoch 2000 Error=0.81778\n",
            "Adam: Epoch 2500 Error=0.81951\n",
            "Adam: Epoch 3000 Error=0.82266\n",
            "Adam: Epoch 3500 Error=0.82737\n",
            "Adam: Epoch 4000 Error=0.83307\n",
            "Adam: Epoch 4500 Error=0.83512\n",
            "Adam: Epoch 5000 Error=0.83806\n",
            "NeuralNetwork(6, [10, 50, 100, 200, 100, 50, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8381\n",
            "Adam: Epoch 500 Error=0.82155\n",
            "Adam: Epoch 1000 Error=0.84943\n",
            "Adam: Epoch 1500 Error=0.86875\n",
            "Adam: Epoch 2000 Error=0.89241\n",
            "Adam: Epoch 2500 Error=0.90887\n",
            "Adam: Epoch 3000 Error=0.92299\n",
            "Adam: Epoch 3500 Error=0.92194\n",
            "Adam: Epoch 4000 Error=0.93013\n",
            "Adam: Epoch 4500 Error=0.93906\n",
            "Adam: Epoch 5000 Error=0.94501\n",
            "NeuralNetwork(6, [200, 100, 50, 10, 50, 100, 200], 7, 'tanh') trained for 5000 epochs, final training error 0.9450\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gLmD9v5AmaO_",
        "outputId": "7462ada9-e402-43b6-f645-80b3f9eec126"
      },
      "source": [
        "for nnet in nnets2:\n",
        "  print(nnet)\n",
        "  Y_classes, Y_probs = nnet.use(Xtest2)\n",
        "  print(\"test\")\n",
        "  confusion_matrix(Y_classes, Ttest2)\n",
        "  Y_classes, Y_probs = nnet.use(Xvalidate2)\n",
        "  print(\"validate\")\n",
        "  confusion_matrix(Y_classes, Tvalidate2)\n",
        "Y_classes, Y_probs = nnets2[0].use(Xtest2)\n",
        "print(\"test\")\n",
        "confusion_matrix(Y_classes, Ttest2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(6, [10], 7, 'tanh') trained for 10000 epochs, final training error 0.8804\n",
            "test\n",
            "Test percent correct 50.00\n",
            "validate\n",
            "Test percent correct 50.00\n",
            "NeuralNetwork(6, [100], 7, 'tanh') trained for 10000 epochs, final training error 0.9944\n",
            "test\n",
            "Test percent correct 46.43\n",
            "validate\n",
            "Test percent correct 46.43\n",
            "NeuralNetwork(6, [10, 20, 10], 7, 'tanh') trained for 10000 epochs, final training error 0.9457\n",
            "test\n",
            "Test percent correct 53.57\n",
            "validate\n",
            "Test percent correct 42.86\n",
            "NeuralNetwork(6, [10, 100], 7, 'tanh') trained for 10000 epochs, final training error 0.9998\n",
            "test\n",
            "Test percent correct 46.43\n",
            "validate\n",
            "Test percent correct 42.86\n",
            "NeuralNetwork(6, [100, 10], 7, 'tanh') trained for 10000 epochs, final training error 0.9230\n",
            "test\n",
            "Test percent correct 46.43\n",
            "validate\n",
            "Test percent correct 39.29\n",
            "NeuralNetwork(6, [100, 10, 100], 7, 'tanh') trained for 10000 epochs, final training error 1.0000\n",
            "test\n",
            "Test percent correct 39.29\n",
            "validate\n",
            "Test percent correct 35.71\n",
            "NeuralNetwork(6, [100, 200, 100], 7, 'tanh') trained for 10000 epochs, final training error 1.0000\n",
            "test\n",
            "Test percent correct 35.71\n",
            "validate\n",
            "Test percent correct 46.43\n",
            "NeuralNetwork(6, [10, 100, 10], 7, 'tanh') trained for 10000 epochs, final training error 0.8678\n",
            "test\n",
            "Test percent correct 21.43\n",
            "validate\n",
            "Test percent correct 32.14\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 7, 'tanh') trained for 10000 epochs, final training error 0.9992\n",
            "test\n",
            "Test percent correct 53.57\n",
            "validate\n",
            "Test percent correct 42.86\n",
            "NeuralNetwork(6, [10, 50, 100, 200, 100, 50, 10], 7, 'tanh') trained for 10000 epochs, final training error 0.8253\n",
            "test\n",
            "Test percent correct 39.29\n",
            "validate\n",
            "Test percent correct 35.71\n",
            "NeuralNetwork(6, [200, 100, 50, 10, 50, 100, 200], 7, 'tanh') trained for 10000 epochs, final training error 0.9755\n",
            "test\n",
            "Test percent correct 46.43\n",
            "validate\n",
            "Test percent correct 32.14\n",
            "test\n",
            "Test percent correct 50.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.0</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>27.272727</td>\n",
              "      <td>9.090909</td>\n",
              "      <td>0.0</td>\n",
              "      <td>63.636364</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           2          3     4           5    6\n",
              "2  80.000000   0.000000   0.0   20.000000  0.0\n",
              "3   0.000000  40.000000   0.0   60.000000  0.0\n",
              "4   0.000000  25.000000  25.0   50.000000  0.0\n",
              "5  27.272727   9.090909   0.0   63.636364  0.0\n",
              "6   0.000000   0.000000   0.0  100.000000  0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Dcqg6XOkc41P",
        "outputId": "22f50a7f-4eda-4582-89cf-24c2f2e96f0a"
      },
      "source": [
        "plt.scatter(range(Y_classes.size),Y_classes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fe752d6b630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "v6Wl_0euesnG",
        "outputId": "271566ce-ea32-488e-e08d-dfb21d87238a"
      },
      "source": [
        "plt.scatter(range(Ttest.size),Ttest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ff38cafdeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COS4Ey4bfHjB",
        "outputId": "999bb553-68a1-4b38-9023-c7ef9c176759"
      },
      "source": [
        "unique, counts = np.unique(T, return_counts=True)\r\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 2284, 4: 1918}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e59MBJ_KfRl6",
        "outputId": "3e0989c0-75c7-4992-a7e4-32a0d1ceb13f"
      },
      "source": [
        "print(T.size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4202\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}