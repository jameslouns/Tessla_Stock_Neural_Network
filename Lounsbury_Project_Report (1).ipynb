{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Lounsbury-Project-Report.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlKoOWxld9Jf"
      },
      "source": [
        "# Using Articles and Elon Musk Tweets to Predict Tesla Stock "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRqellard9Jn"
      },
      "source": [
        "*by James Lounsbury, December 16, 2020*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgrrPXU2d9Jp"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxE5V7T_d9Jp"
      },
      "source": [
        "The Big data portion of this project was worked on by my team (Gabriel Vigil and Reece Sharp) for CS435. This Report will focus on the Neural Nework I created for modeling the stock market.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8B_iH6rd9Jp"
      },
      "source": [
        "The stock market is increadibly complex system that is very difficult to model. Stocks are constanly fluctuating due to countless unseen variables.For companies that are regularly in the public eye like tesla, public opinion can be a major factor. Because of this to try to improve a network modeling tesla stock we would like to try to account for this. To do this we took many data sets of news articles and filtered for only article titles that include elon or tesla. We also scraped twiter for all elon musk tweets and replies to his tweets. We then prefored sentiment analysis on the articles titles and tweets to determine if they were positive or negitive. Because stock dont only depend on their own factors but also depend on how the overall market is preforming we needed to account for this. This was accomplished by collecting data from the Dow Jones industrial Average. For the Dow Jones we found the difference between the previous days opening price and the current days opening price and this was put for the current day. Finaly for the target value, the difference between the tesla stock current days opening and next days closing was put in the current day. These datasets were then joined on thier datas. \r\n",
        "\r\n",
        "Because attemping to create a model to predict the exact difference in the stocks seemed to ambitious for our data sets, the tesla difference was broken up into eight catagories. Four catagories were for positive change and four of negitive these being:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5e9CKK7dTH11",
        "outputId": "f2c4f925-c315-44e2-d132-30f30e48ffee"
      },
      "source": [
        "import pandas \r\n",
        "F = {'%': ['>10','5 to 10','1 to 5','0 to 1','0 to -1','-1 to -5','-5 to -10','<-10'],'Classifier': [0,1,2,3,4,5,6,7]}\r\n",
        "df=pandas.DataFrame(data=F,index=['','','','','','','',''])\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>%</th>\n",
              "      <th>Classifier</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>&gt;10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>5 to 10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>1 to 5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>0 to 1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>0 to -1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>-1 to -5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>-5 to -10</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>&lt;-10</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          %  Classifier\n",
              "        >10           0\n",
              "    5 to 10           1\n",
              "     1 to 5           2\n",
              "     0 to 1           3\n",
              "    0 to -1           4\n",
              "   -1 to -5           5\n",
              "  -5 to -10           6\n",
              "       <-10           7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmvgIOpYe1j-"
      },
      "source": [
        "## Code: used for graphics in Methods and Results; doesnt need to be run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm5FGhk8fGVX"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.autograd import Variable\r\n",
        "import time\r\n",
        "import pandas\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDfOXVK8jDMt"
      },
      "source": [
        "def Kfold_partition(X, T, n_folds, random_shuffle=True):\r\n",
        "    np.random.seed(42)\r\n",
        "    rows = np.arange(X.shape[0])\r\n",
        "    np.random.shuffle(rows)\r\n",
        "    X = X[rows, :]\r\n",
        "    T = T[rows, :]\r\n",
        "    \r\n",
        "    n_samples = X.shape[0]\r\n",
        "    n_per_fold = n_samples // n_folds\r\n",
        "    n_last_fold = n_samples - n_per_fold * (n_folds - 1)  \r\n",
        "\r\n",
        "    folds = []\r\n",
        "    start = 0\r\n",
        "    for foldi in range(n_folds-1):\r\n",
        "        folds.append( (X[start:start + n_per_fold, :], T[start:start + n_per_fold, :]) )\r\n",
        "        start += n_per_fold\r\n",
        "    folds.append( (X[start:, :], T[start:, :]) )\r\n",
        "    len(folds), len(folds[0]), folds[0][0].shape, folds[0][1].shape\r\n",
        "    Xvalidate, Tvalidate = folds[0]\r\n",
        "    Xtest, Ttest = folds[1]\r\n",
        "    Xtrain, Ttrain = np.vstack([X for (X, _) in folds[2:]]), np.vstack([T for (_, T) in folds[2:]])\r\n",
        "    Xtrain.shape, Ttrain.shape, Xvalidate.shape, Tvalidate.shape, Xtest.shape, Ttest.shape\r\n",
        "\r\n",
        "    return [Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rih6P_J8lIF-"
      },
      "source": [
        "def confusion_matrix(Y_classes, T):\r\n",
        "  class_names=np.unique(T)\r\n",
        "  table = []\r\n",
        "  for true_class in class_names:\r\n",
        "    row = []\r\n",
        "    for predicted_class in class_names:\r\n",
        "        row.append(100 * np.mean(Y_classes[T == true_class] == predicted_class))\r\n",
        "    table.append(row)\r\n",
        "    \r\n",
        "  print(f'Test percent correct {np.mean(Y_classes == T) * 100:.2f}') \r\n",
        "\r\n",
        "  return pandas.DataFrame(table, index=class_names, columns=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIeBkQyolRIR",
        "outputId": "de03bce3-c6f7-48fe-9e61-6a04c7e5a0e3"
      },
      "source": [
        "%%writefile optimizers.py\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class Optimizers():\r\n",
        "\r\n",
        "    def __init__(self, all_weights):\r\n",
        "        \r\n",
        "        self.all_weights = all_weights\r\n",
        "\r\n",
        "        self.mt = np.zeros_like(all_weights)\r\n",
        "        self.vt = np.zeros_like(all_weights)\r\n",
        "        self.beta1 = 0.9\r\n",
        "        self.beta2 = 0.999\r\n",
        "        self.beta1t = 1\r\n",
        "        self.beta2t = 1\r\n",
        "\r\n",
        "        \r\n",
        "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=False, error_convert_f=None):\r\n",
        "\r\n",
        "        error_trace = []\r\n",
        "        epochs_per_print = n_epochs // 10\r\n",
        "\r\n",
        "        for epoch in range(n_epochs):\r\n",
        "\r\n",
        "            error = error_f(*fargs)\r\n",
        "            grad = gradient_f(*fargs)\r\n",
        "\r\n",
        "            # Update all weights using -= to modify their values in-place.\r\n",
        "            self.all_weights -= learning_rate * grad\r\n",
        "\r\n",
        "            if error_convert_f:\r\n",
        "                error = error_convert_f(error)\r\n",
        "            error_trace.append(error)\r\n",
        "\r\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\r\n",
        "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\r\n",
        "\r\n",
        "        return error_trace\r\n",
        "\r\n",
        "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=False, error_convert_f=None):\r\n",
        "\r\n",
        "        alpha = learning_rate  # learning rate called alpha in original paper on adam\r\n",
        "        epsilon = 1e-8\r\n",
        "        error_trace = []\r\n",
        "        epochs_per_print = n_epochs // 10\r\n",
        "\r\n",
        "        for epoch in range(n_epochs):\r\n",
        "\r\n",
        "            error = error_f(*fargs)\r\n",
        "            grad = gradient_f(*fargs)\r\n",
        "\r\n",
        "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\r\n",
        "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\r\n",
        "            self.beta1t *= self.beta1\r\n",
        "            self.beta2t *= self.beta2\r\n",
        "\r\n",
        "            m_hat = self.mt / (1 - self.beta1t)\r\n",
        "            v_hat = self.vt / (1 - self.beta2t)\r\n",
        "\r\n",
        "            # Update all weights using -= to modify their values in-place.\r\n",
        "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\r\n",
        "    \r\n",
        "            if error_convert_f:\r\n",
        "                error = error_convert_f(error)\r\n",
        "            error_trace.append(error)\r\n",
        "\r\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\r\n",
        "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\r\n",
        "\r\n",
        "        return error_trace\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    plt.ion()\r\n",
        "\r\n",
        "    def parabola(wmin):\r\n",
        "        return ((w - wmin) ** 2)[0]\r\n",
        "\r\n",
        "    def parabola_gradient(wmin):\r\n",
        "        return 2 * (w - wmin)\r\n",
        "\r\n",
        "    w = np.array([0.0])\r\n",
        "    optimizer = Optimizers(w)\r\n",
        "\r\n",
        "    wmin = 5\r\n",
        "    optimizer.sgd(parabola, parabola_gradient, [wmin],\r\n",
        "                  n_epochs=500, learning_rate=0.1)\r\n",
        "\r\n",
        "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\r\n",
        "\r\n",
        "    w = np.array([0.0])\r\n",
        "    optimizer = Optimizers(w)\r\n",
        "    optimizer.adam(parabola, parabola_gradient, [wmin],\r\n",
        "                   n_epochs=500, learning_rate=0.1)\r\n",
        "    \r\n",
        "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting optimizers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqM5YYdmVEB"
      },
      "source": [
        "import optimizers\r\n",
        "import sys  \r\n",
        "\r\n",
        "class NeuralNetwork():\r\n",
        "\r\n",
        "\r\n",
        "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\r\n",
        "        self.n_inputs = n_inputs\r\n",
        "        self.n_outputs = n_outputs\r\n",
        "        self.activation_function = activation_function\r\n",
        "\r\n",
        "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\r\n",
        "            self.n_hiddens_per_layer = []\r\n",
        "        else:\r\n",
        "            self.n_hiddens_per_layer = n_hiddens_per_layer\r\n",
        "\r\n",
        "        n_in = n_inputs\r\n",
        "        shapes = []\r\n",
        "        for nh in self.n_hiddens_per_layer:\r\n",
        "            shapes.append((n_in + 1, nh))\r\n",
        "            n_in = nh\r\n",
        "        shapes.append((n_in + 1, n_outputs))\r\n",
        "\r\n",
        "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\r\n",
        "\r\n",
        "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\r\n",
        "\r\n",
        "        self.trained = False\r\n",
        "        self.total_epochs = 0\r\n",
        "        self.error_trace = []\r\n",
        "        self.Xmeans = None\r\n",
        "        self.Xstds = None\r\n",
        "        self.Tmeans = None\r\n",
        "        self.Tstds = None\r\n",
        "\r\n",
        "\r\n",
        "    def make_weights_and_views(self, shapes):\r\n",
        "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\r\n",
        "                                 for shape in shapes])\r\n",
        "        views = []\r\n",
        "        start = 0\r\n",
        "        for shape in shapes:\r\n",
        "            size =shape[0] * shape[1]\r\n",
        "            views.append(all_weights[start:start + size].reshape(shape))\r\n",
        "            start += size\r\n",
        "        return all_weights, views\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        result = self.__repr__()\r\n",
        "        if len(self.error_trace) > 0:\r\n",
        "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\r\n",
        "    def makeIndicatorVars(self, T):\r\n",
        "          if T.ndim == 1:\r\n",
        "              T = T.reshape((-1, 1))\r\n",
        "          retT = (T == np.unique(T)).astype(int)\r\n",
        "          return retT\r\n",
        "          \r\n",
        "    def softmax (self, X):\r\n",
        "        fs = np.exp(X)  \r\n",
        "        denom = np.sum(fs, axis=1).reshape((-1, 1))\r\n",
        "        gs = fs / (denom + sys.float_info.epsilon)\r\n",
        "        return gs\r\n",
        "\r\n",
        "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\r\n",
        "        self.verbose=verbose\r\n",
        "        if self.Xmeans is None:\r\n",
        "            self.Xmeans = X.mean(axis=0)\r\n",
        "            self.Xstds = X.std(axis=0)\r\n",
        "            self.Xstds[self.Xstds == 0] = 1  \r\n",
        "            self.Tmeans = T.mean(axis=0)\r\n",
        "            self.Tstds = T.std(axis=0)\r\n",
        "            \r\n",
        "        X = (X - self.Xmeans) / self.Xstds\r\n",
        "        self.TtrainI = self.makeIndicatorVars(T)\r\n",
        "        self.uniqueT=np.unique(T)\r\n",
        "        optimizer = optimizers.Optimizers(self.all_weights)\r\n",
        "\r\n",
        "\r\n",
        "        error_convert_f = lambda nll: (np.exp(-nll))\r\n",
        "\r\n",
        "        if method == 'sgd':\r\n",
        "\r\n",
        "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\r\n",
        "                                        fargs=[X, self.TtrainI], n_epochs=n_epochs,\r\n",
        "                                        learning_rate=learning_rate,\r\n",
        "                                        verbose=self.verbose,\r\n",
        "                                        error_convert_f=error_convert_f)\r\n",
        "\r\n",
        "        elif method == 'adam':\r\n",
        "\r\n",
        "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\r\n",
        "                                         fargs=[X, self.TtrainI], n_epochs=n_epochs,\r\n",
        "                                         learning_rate=learning_rate,\r\n",
        "                                         verbose=self.verbose,\r\n",
        "                                         error_convert_f=error_convert_f)\r\n",
        "\r\n",
        "        else:\r\n",
        "            raise Exception(\"method must be 'sgd' or 'adam'\")\r\n",
        "        \r\n",
        "        self.error_trace = error_trace\r\n",
        "\r\n",
        "        return self\r\n",
        "\r\n",
        "    def relu(self, s):\r\n",
        "        s[s < 0] = 0\r\n",
        "        return s\r\n",
        "\r\n",
        "    def grad_relu(self, s):\r\n",
        "        return (s > 0).astype(int)\r\n",
        "    \r\n",
        "    def forward_pass(self, X):\r\n",
        "        self.Ys = [X]\r\n",
        "        for W in self.Ws[:-1]:\r\n",
        "            if self.activation_function == 'relu':\r\n",
        "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\r\n",
        "            else:\r\n",
        "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\r\n",
        "        last_W = self.Ws[-1]\r\n",
        "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\r\n",
        "        return self.Ys\r\n",
        "\r\n",
        "    def error_f(self, X, T):\r\n",
        "        temp = self.forward_pass(X)\r\n",
        "        Y = self.softmax(temp[-1])\r\n",
        "        return - np.mean((T * np.log(Y)))\r\n",
        "\r\n",
        "    def gradient_f(self, X, T):\r\n",
        "        Y = self.softmax(self.Ys[-1])\r\n",
        "        delta = (Y - T) / (T.shape[0] * T.shape[1])\r\n",
        "        n_layers = len(self.n_hiddens_per_layer) + 1\r\n",
        "        for layeri in range(n_layers - 1, -1, -1):\r\n",
        "              self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\r\n",
        "              self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\r\n",
        "              if self.activation_function == 'relu':\r\n",
        "                  delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\r\n",
        "              else:\r\n",
        "                  delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\r\n",
        "        return self.all_gradients\r\n",
        "\r\n",
        "    def use(self, X):\r\n",
        "        Xtest = ((X - self.Xmeans)/self.Xstds)\r\n",
        "        temp = self.forward_pass(Xtest)\r\n",
        "        logregOutput = self.softmax(temp[-1])\r\n",
        "        predictedTrain = np.argmax(logregOutput,axis=1)\r\n",
        "        result = np.array(list(map(lambda x: [self.uniqueT[x].astype(int)] , predictedTrain)))\r\n",
        "        return result , logregOutput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p7-A-45Aq7k"
      },
      "source": [
        "**If running code use attached file bellow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "x1NOO7KJAXqk",
        "outputId": "0462266e-1635-4b5c-ec36-f55613d69d5e"
      },
      "source": [
        "from google.colab import files\r\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c16180bd-5c33-408e-b0e1-00022da949e3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c16180bd-5c33-408e-b0e1-00022da949e3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Team_Project_Final_Dataset.csv to Team_Project_Final_Dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya2qKR7IAhY8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d1d64d58-7b2e-4993-86af-52b3fa579221"
      },
      "source": [
        "FileData = pandas.read_csv('Team_Project_Final_Dataset.csv', delimiter=',',\r\n",
        "                          names=('Date', '# of Tweets', '# of +Tweets','# of -Tweets',\r\n",
        "                                 '# of +Articles', '# of -Articles', '% Diff Dow Jones',\r\n",
        "                                 '% Diff Tesla'))\r\n",
        "FileData = FileData.drop([0])\r\n",
        "FileData['# of Tweets'] = FileData['# of Tweets'].astype(float)\r\n",
        "FileData['# of +Tweets'] = FileData['# of +Tweets'].astype(float)\r\n",
        "FileData['# of -Tweets'] = FileData['# of -Tweets'].astype(float)\r\n",
        "FileData['# of +Articles'] = FileData['# of +Articles'].astype(float)\r\n",
        "FileData['# of -Articles'] = FileData['# of -Articles'].astype(float)\r\n",
        "FileData['% Diff Dow Jones'] = FileData['% Diff Dow Jones'].astype(float)\r\n",
        "FileData['% Diff Tesla'] = FileData['% Diff Tesla'].astype(int)\r\n",
        "FileData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th># of Tweets</th>\n",
              "      <th># of +Tweets</th>\n",
              "      <th># of -Tweets</th>\n",
              "      <th># of +Articles</th>\n",
              "      <th># of -Articles</th>\n",
              "      <th>% Diff Dow Jones</th>\n",
              "      <th>% Diff Tesla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-09-27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.003653</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013-06-28</td>\n",
              "      <td>109.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013-06-29</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013-06-30</td>\n",
              "      <td>23.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.002772</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2013-07-15</td>\n",
              "      <td>525.0</td>\n",
              "      <td>791.0</td>\n",
              "      <td>620.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4199</th>\n",
              "      <td>2011-10-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010974</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4200</th>\n",
              "      <td>2011-10-15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4201</th>\n",
              "      <td>2011-10-16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.007141</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4202</th>\n",
              "      <td>2012-03-22</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4203</th>\n",
              "      <td>2011-08-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007059</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4203 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  # of Tweets  ...  % Diff Dow Jones  % Diff Tesla\n",
              "1     2018-09-27          NaN  ...         -0.003653             7\n",
              "2     2013-06-28        109.0  ...          0.000000             1\n",
              "3     2013-06-29          NaN  ...         -0.002772             1\n",
              "4     2013-06-30         23.0  ...         -0.002772             1\n",
              "5     2013-07-15        525.0  ...          0.001524             7\n",
              "...          ...          ...  ...               ...           ...\n",
              "4199  2011-10-14          NaN  ...          0.010974             5\n",
              "4200  2011-10-15          NaN  ...         -0.007141             5\n",
              "4201  2011-10-16          NaN  ...         -0.007141             5\n",
              "4202  2012-03-22          3.0  ...         -0.009464             5\n",
              "4203  2011-08-31          NaN  ...          0.007059             5\n",
              "\n",
              "[4203 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-TAeiFFBJIZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "3b1a27d6-1051-4867-a515-c0dd553caa32"
      },
      "source": [
        "AllCat=[FileData.dropna()]\r\n",
        "AllCat.append(FileData.drop(['% Diff Dow Jones'],axis=1).dropna())\r\n",
        "AllCat.append(FileData.drop(['# of -Tweets','# of -Articles'],axis=1).dropna())\r\n",
        "AllCat.append(FileData.drop(['# of -Tweets','# of -Articles'],axis=1).dropna())\r\n",
        "TwoCat=[]\r\n",
        "for i in AllCat:\r\n",
        "  TwoCat.append(i.replace({'% Diff Tesla': {1: 0,2: 0,3: 0,5: 4,6: 4,7: 4}}).replace({'% Diff Tesla': {4:1}}))\r\n",
        "AllCat[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th># of Tweets</th>\n",
              "      <th># of +Tweets</th>\n",
              "      <th># of -Tweets</th>\n",
              "      <th># of +Articles</th>\n",
              "      <th># of -Articles</th>\n",
              "      <th>% Diff Dow Jones</th>\n",
              "      <th>% Diff Tesla</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2016-07-21</td>\n",
              "      <td>361.0</td>\n",
              "      <td>527.0</td>\n",
              "      <td>376.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.003523</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2016-07-21</td>\n",
              "      <td>1183.0</td>\n",
              "      <td>1172.0</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.003523</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2017-05-13</td>\n",
              "      <td>1354.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>1192.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.004248</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>2017-05-13</td>\n",
              "      <td>541.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.004248</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>2017-04-07</td>\n",
              "      <td>750.0</td>\n",
              "      <td>361.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4187</th>\n",
              "      <td>2016-03-31</td>\n",
              "      <td>647.0</td>\n",
              "      <td>657.0</td>\n",
              "      <td>487.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4188</th>\n",
              "      <td>2016-03-31</td>\n",
              "      <td>173.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4189</th>\n",
              "      <td>2016-03-31</td>\n",
              "      <td>615.0</td>\n",
              "      <td>543.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4190</th>\n",
              "      <td>2016-03-31</td>\n",
              "      <td>243.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4192</th>\n",
              "      <td>2016-05-19</td>\n",
              "      <td>92.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.003764</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>287 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  # of Tweets  ...  % Diff Dow Jones  % Diff Tesla\n",
              "25    2016-07-21        361.0  ...         -0.003523             5\n",
              "26    2016-07-21       1183.0  ...         -0.003523             5\n",
              "37    2017-05-13       1354.0  ...          0.004248             5\n",
              "38    2017-05-13        541.0  ...          0.004248             5\n",
              "112   2017-04-07        750.0  ...          0.000000             1\n",
              "...          ...          ...  ...               ...           ...\n",
              "4187  2016-03-31        647.0  ...          0.001854             2\n",
              "4188  2016-03-31        173.0  ...          0.001854             2\n",
              "4189  2016-03-31        615.0  ...          0.001854             2\n",
              "4190  2016-03-31        243.0  ...          0.001854             2\n",
              "4192  2016-05-19         92.0  ...         -0.003764             2\n",
              "\n",
              "[287 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAWHHNZ_YLxB"
      },
      "source": [
        "AllCatX_T=[]\r\n",
        "for i in AllCat:\r\n",
        "  AllCatX_T.append(Kfold_partition(i.iloc[:,1:-1].to_numpy(), i.iloc[:,-1:].to_numpy(), 10, random_shuffle=True))\r\n",
        "TwoCatX_T=[]\r\n",
        "for i in TwoCat:\r\n",
        "  TwoCatX_T.append(Kfold_partition(i.iloc[:,1:-1].to_numpy(), i.iloc[:,-1:].to_numpy(), 10, random_shuffle=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zc4T9A9fbUD"
      },
      "source": [
        "AllCatNnets=[]\r\n",
        "TwoCatNnets=[]\r\n",
        "for i in range(4):\r\n",
        "  AllCatNnets.append([])\r\n",
        "  TwoCatNnets.append([])\r\n",
        "  for j in [[10],[100],[10,20,10],[10,100],[100,10],[100,10,100],[100,200,100],[10,100,10],[20,50,100,50,20]]:\r\n",
        "    AllCatNnets[i].append(NeuralNetwork(AllCatX_T[i][0].shape[1], j, len(np.unique(AllCatX_T[i][1]))))\r\n",
        "    TwoCatNnets[i].append(NeuralNetwork(TwoCatX_T[i][0].shape[1], j, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF-I0tOWlGk7",
        "outputId": "f9b5e320-7786-4b51-8c09-5cabc597244d"
      },
      "source": [
        "for i in range(4):\r\n",
        "  for nnet in AllCatNnets[i]:\r\n",
        "    nnet.train(AllCatX_T[i][0], AllCatX_T[i][1], 5000, .0005, method='adam', verbose=False)\r\n",
        "    print(nnet)\r\n",
        "for i in range(4):\r\n",
        "  for nnet in TwoCatNnets[i]:\r\n",
        "    nnet.train(TwoCatX_T[i][0], TwoCatX_T[i][1], 5000, .0005, method='adam', verbose=False)\r\n",
        "    print(nnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(6, [10], 7, 'tanh') trained for 5000 epochs, final training error 0.8569\n",
            "NeuralNetwork(6, [100], 7, 'tanh') trained for 5000 epochs, final training error 0.9534\n",
            "NeuralNetwork(6, [10, 20, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8733\n",
            "NeuralNetwork(6, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9910\n",
            "NeuralNetwork(6, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8726\n",
            "NeuralNetwork(6, [100, 10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9875\n",
            "NeuralNetwork(6, [100, 200, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9998\n",
            "NeuralNetwork(6, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8575\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.9764\n",
            "NeuralNetwork(5, [10], 7, 'tanh') trained for 5000 epochs, final training error 0.8280\n",
            "NeuralNetwork(5, [100], 7, 'tanh') trained for 5000 epochs, final training error 0.8792\n",
            "NeuralNetwork(5, [10, 20, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8594\n",
            "NeuralNetwork(5, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9431\n",
            "NeuralNetwork(5, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8607\n",
            "NeuralNetwork(5, [100, 10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9832\n",
            "NeuralNetwork(5, [100, 200, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9958\n",
            "NeuralNetwork(5, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8564\n",
            "NeuralNetwork(5, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.8095\n",
            "NeuralNetwork(4, [10], 7, 'tanh') trained for 5000 epochs, final training error 0.8382\n",
            "NeuralNetwork(4, [100], 7, 'tanh') trained for 5000 epochs, final training error 0.9148\n",
            "NeuralNetwork(4, [10, 20, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8744\n",
            "NeuralNetwork(4, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9757\n",
            "NeuralNetwork(4, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8720\n",
            "NeuralNetwork(4, [100, 10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9866\n",
            "NeuralNetwork(4, [100, 200, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9966\n",
            "NeuralNetwork(4, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8523\n",
            "NeuralNetwork(4, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.9491\n",
            "NeuralNetwork(4, [10], 7, 'tanh') trained for 5000 epochs, final training error 0.8414\n",
            "NeuralNetwork(4, [100], 7, 'tanh') trained for 5000 epochs, final training error 0.9174\n",
            "NeuralNetwork(4, [10, 20, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8351\n",
            "NeuralNetwork(4, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9708\n",
            "NeuralNetwork(4, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8600\n",
            "NeuralNetwork(4, [100, 10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9401\n",
            "NeuralNetwork(4, [100, 200, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9945\n",
            "NeuralNetwork(4, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8617\n",
            "NeuralNetwork(4, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.8682\n",
            "NeuralNetwork(6, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.8372\n",
            "NeuralNetwork(6, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.9552\n",
            "NeuralNetwork(6, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9792\n",
            "NeuralNetwork(6, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9736\n",
            "NeuralNetwork(6, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9372\n",
            "NeuralNetwork(6, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9562\n",
            "NeuralNetwork(6, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7438\n",
            "NeuralNetwork(6, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.7618\n",
            "NeuralNetwork(6, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.7356\n",
            "NeuralNetwork(5, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.7762\n",
            "NeuralNetwork(5, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.8230\n",
            "NeuralNetwork(5, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8667\n",
            "NeuralNetwork(5, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.8702\n",
            "NeuralNetwork(5, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8634\n",
            "NeuralNetwork(5, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.8552\n",
            "NeuralNetwork(5, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7351\n",
            "NeuralNetwork(5, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8077\n",
            "NeuralNetwork(5, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.7309\n",
            "NeuralNetwork(4, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.7839\n",
            "NeuralNetwork(4, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.8771\n",
            "NeuralNetwork(4, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8710\n",
            "NeuralNetwork(4, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9398\n",
            "NeuralNetwork(4, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8321\n",
            "NeuralNetwork(4, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9539\n",
            "NeuralNetwork(4, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7311\n",
            "NeuralNetwork(4, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.7487\n",
            "NeuralNetwork(4, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.8557\n",
            "NeuralNetwork(4, [10], 2, 'tanh') trained for 5000 epochs, final training error 0.7843\n",
            "NeuralNetwork(4, [100], 2, 'tanh') trained for 5000 epochs, final training error 0.8704\n",
            "NeuralNetwork(4, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9009\n",
            "NeuralNetwork(4, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9750\n",
            "NeuralNetwork(4, [100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8623\n",
            "NeuralNetwork(4, [100, 10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9274\n",
            "NeuralNetwork(4, [100, 200, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.7675\n",
            "NeuralNetwork(4, [10, 100, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.7535\n",
            "NeuralNetwork(4, [20, 50, 100, 50, 20], 2, 'tanh') trained for 5000 epochs, final training error 0.7424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie60H6Tkvkj_"
      },
      "source": [
        "def numberCorrect(nnet,X,T):\r\n",
        "  Y_classes, probs = nnet.use(X)\r\n",
        "  return np.sum(T==Y_classes)\r\n",
        "\r\n",
        "def bestNet(nnets,Xtest,Ttest,Xval,Tval):\r\n",
        "  bestaverage=0\r\n",
        "  bestIndex=0\r\n",
        "  for i in range(len(nnets)):\r\n",
        "    curAvg=(numberCorrect(nnets[i],Xtest,Ttest)+numberCorrect(nnets[i],Xval,Tval))/(len(Ttest)+len(Tval))\r\n",
        "    if bestaverage<curAvg:\r\n",
        "      bestaverage=curAvg\r\n",
        "      bestIndex=i\r\n",
        "  return bestIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1rxLD2U1lnl"
      },
      "source": [
        "bestAllNets=[]\r\n",
        "for i in range(4):\r\n",
        "  index=bestNet(AllCatNnets[i],AllCatX_T[i][2],AllCatX_T[i][3],AllCatX_T[i][4],AllCatX_T[i][5])\r\n",
        "  bestAllNets.append(AllCatNnets[i][index])\r\n",
        "bestTwoNets=[]\r\n",
        "for i in range(4):\r\n",
        "  index=bestNet(TwoCatNnets[i],TwoCatX_T[i][2],TwoCatX_T[i][3],TwoCatX_T[i][4],TwoCatX_T[i][5])\r\n",
        "  bestTwoNets.append(TwoCatNnets[i][index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FGyYT_935ig",
        "outputId": "bf90728e-8167-410a-caa4-44dbac4d88b1"
      },
      "source": [
        "for i in range(4):\r\n",
        "  print(i)\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"All Catagories\")\r\n",
        "  print(bestAllNets[i])\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][5]))\r\n",
        "  print()\r\n",
        "  print(\"Two Catagories\")\r\n",
        "  print(bestTwoNets[i])\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(6, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9910\n",
            "Test\n",
            "Test percent correct 50.00\n",
            "      1          2           3          4          5    6\n",
            "1  50.0   0.000000    0.000000   0.000000  50.000000  0.0\n",
            "2   0.0  42.857143   14.285714  14.285714  28.571429  0.0\n",
            "3   0.0   0.000000   60.000000   0.000000  40.000000  0.0\n",
            "4   0.0   0.000000   20.000000  40.000000  40.000000  0.0\n",
            "5   0.0  16.666667    0.000000   0.000000  66.666667  0.0\n",
            "6   0.0   0.000000  100.000000   0.000000   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 50.00\n",
            "           2          3     4          5         6\n",
            "2  60.000000   0.000000  20.0  20.000000  0.000000\n",
            "3   0.000000  60.000000   0.0  40.000000  0.000000\n",
            "4  25.000000   0.000000  50.0  25.000000  0.000000\n",
            "5  18.181818   9.090909   0.0  54.545455  9.090909\n",
            "6   0.000000  33.333333   0.0  66.666667  0.000000\n",
            "\n",
            "Two Catagories\n",
            "NeuralNetwork(6, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9792\n",
            "Test\n",
            "Test percent correct 67.86\n",
            "           0          1\n",
            "0  75.000000  25.000000\n",
            "1  41.666667  58.333333\n",
            "\n",
            "Validate\n",
            "Test percent correct 75.00\n",
            "           0          1\n",
            "0  80.000000  20.000000\n",
            "1  27.777778  72.222222\n",
            "--------------------------------------------------\n",
            "1\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(5, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.8095\n",
            "Test\n",
            "Test percent correct 41.38\n",
            "     1           2          3    4           5    6\n",
            "1  0.0  100.000000   0.000000  0.0    0.000000  0.0\n",
            "2  0.0   85.714286   0.000000  0.0   14.285714  0.0\n",
            "3  0.0    0.000000  20.000000  0.0   80.000000  0.0\n",
            "4  0.0   33.333333  16.666667  0.0   50.000000  0.0\n",
            "5  0.0   37.500000   0.000000  0.0   62.500000  0.0\n",
            "6  0.0    0.000000   0.000000  0.0  100.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 41.38\n",
            "     1          2     3    4           5    6\n",
            "1  0.0   0.000000   0.0  0.0  100.000000  0.0\n",
            "2  0.0  50.000000   0.0  0.0   50.000000  0.0\n",
            "3  0.0  25.000000  25.0  0.0   50.000000  0.0\n",
            "4  0.0  66.666667   0.0  0.0   33.333333  0.0\n",
            "5  0.0  42.857143   0.0  0.0   57.142857  0.0\n",
            "6  0.0   0.000000   0.0  0.0  100.000000  0.0\n",
            "\n",
            "Two Catagories\n",
            "NeuralNetwork(5, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.8702\n",
            "Test\n",
            "Test percent correct 51.72\n",
            "           0          1\n",
            "0  53.846154  46.153846\n",
            "1  50.000000  50.000000\n",
            "\n",
            "Validate\n",
            "Test percent correct 58.62\n",
            "           0          1\n",
            "0  54.545455  45.454545\n",
            "1  38.888889  61.111111\n",
            "--------------------------------------------------\n",
            "2\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(4, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8720\n",
            "Test\n",
            "Test percent correct 42.86\n",
            "      1          2          3           4          5    6\n",
            "1  50.0   0.000000   0.000000   25.000000  25.000000  0.0\n",
            "2   0.0  42.857143   0.000000   14.285714  42.857143  0.0\n",
            "3   0.0  40.000000  40.000000   20.000000   0.000000  0.0\n",
            "4   0.0   0.000000  20.000000   40.000000  40.000000  0.0\n",
            "5   0.0   0.000000  16.666667   16.666667  50.000000  0.0\n",
            "6   0.0   0.000000   0.000000  100.000000   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 42.86\n",
            "           2          3          4          5    6\n",
            "2  40.000000  60.000000   0.000000   0.000000  0.0\n",
            "3   0.000000  40.000000  40.000000  20.000000  0.0\n",
            "4  25.000000   0.000000  75.000000   0.000000  0.0\n",
            "5   9.090909   9.090909  36.363636  45.454545  0.0\n",
            "6   0.000000   0.000000  33.333333  66.666667  0.0\n",
            "\n",
            "Two Catagories\n",
            "NeuralNetwork(4, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8710\n",
            "Test\n",
            "Test percent correct 60.71\n",
            "           0          1\n",
            "0  56.250000  43.750000\n",
            "1  33.333333  66.666667\n",
            "\n",
            "Validate\n",
            "Test percent correct 71.43\n",
            "           0          1\n",
            "0  50.000000  50.000000\n",
            "1  16.666667  83.333333\n",
            "--------------------------------------------------\n",
            "3\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(4, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8617\n",
            "Test\n",
            "Test percent correct 35.71\n",
            "      1          2           3    4          5    6\n",
            "1  50.0   0.000000    0.000000  0.0  50.000000  0.0\n",
            "2   0.0  57.142857    0.000000  0.0  42.857143  0.0\n",
            "3   0.0  20.000000   20.000000  0.0  60.000000  0.0\n",
            "4   0.0   0.000000   60.000000  0.0  40.000000  0.0\n",
            "5   0.0  16.666667   16.666667  0.0  50.000000  0.0\n",
            "6   0.0   0.000000  100.000000  0.0   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 46.43\n",
            "           2          3    4          5    6\n",
            "2  40.000000  40.000000  0.0  20.000000  0.0\n",
            "3   0.000000  60.000000  0.0  40.000000  0.0\n",
            "4  50.000000  25.000000  0.0  25.000000  0.0\n",
            "5  18.181818   9.090909  0.0  72.727273  0.0\n",
            "6   0.000000  33.333333  0.0  66.666667  0.0\n",
            "\n",
            "Two Catagories\n",
            "NeuralNetwork(4, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9750\n",
            "Test\n",
            "Test percent correct 71.43\n",
            "           0          1\n",
            "0  75.000000  25.000000\n",
            "1  33.333333  66.666667\n",
            "\n",
            "Validate\n",
            "Test percent correct 53.57\n",
            "           0          1\n",
            "0  40.000000  60.000000\n",
            "1  38.888889  61.111111\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z6lVcZwSqGT"
      },
      "source": [
        "## Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urwigHjiSyC9"
      },
      "source": [
        "Since I decided to break up the tesla stock data into catagories the only correct model to use was a classification Neural network. For this i reused the classification network from Hw Four. For the optimizer i use the Adam implimentaion from the same homework. To ensure the data was broken up into a random sets for training and testing I used K-fold partioning. \r\n",
        "\r\n",
        "To test if our the theory about public opinion being a useful feature to use in the modeling of stock data; i trained multiple network structures with and without some features. The sets of features i train were, the entire feature list, the feature list excluding the dow jones, the feature list excluding the negitive sentiment analysis, and the feature set excluding the positve sentiment analyis. For ensuring that all feature sets had a decient chance to create a accurate model, I trained a set of multiple networks for each feature set. \r\n",
        "\r\n",
        "This set being \r\n",
        "\r\n",
        " [ [10],\r\n",
        " [100],\r\n",
        " [10,20,10],\r\n",
        " [10,100],\r\n",
        " [100,10],\r\n",
        " [100,10,100],\r\n",
        " [100,200,100],\r\n",
        " [10,100,10],\r\n",
        " [20,50,100,50,20],\r\n",
        " [10,50,100,200,100,50,10],\r\n",
        " [200,100,50,10,50,100,200] ]\r\n",
        "\r\n",
        "These were all train for 5000 epochs with a learning rate of .0005 because in inital trials this gave the most accurate values on average. Once all the networks were trained I ran each set through a function to find the best network for that set. This was done by summing the number of correct classifications for both the test and validate data set and dividing by the sum of their sizes. The network with the highest value was then conisdered the best.\r\n",
        "\r\n",
        "After running this a problem became clear with our data set. For the catagory system that we choose the distribution was no where neer evenly distributed.\r\n",
        "\r\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "W4mLuIXDjb1t",
        "outputId": "090d3c9f-b312-4f2a-c451-22c433fd6aa8"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.scatter(range(AllCatX_T[0][1].size),AllCatX_T[0][1])\r\n",
        "plt.ylabel('Catagory')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Catagory')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lWrp8ABkI0N"
      },
      "source": [
        "As the plot above shows our data set is focused in the 2 and 5 catagories with little to no points in the 0-1 and 6-7 ranges. This was heavily reflected in our models, with the all the neworks only guessing between the range of 2-5.\r\n",
        "To solve this without having to rerun the join on the inital data set, I decided to do a mapping of the original catagories to a new set. The New set contained only two catagories, 0 for positive price change and 1 for negitive price change. This was simply done by mapping all catagories < 4 to 0 and 4 <= catagories to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "R_pdrnmoobjG",
        "outputId": "fcbd4273-494a-4ad8-ead6-8a1996ef74c4"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.scatter(range(TwoCatX_T[0][1].size),TwoCatX_T[0][1])\r\n",
        "plt.ylabel('Catagory')\r\n",
        "print(\"Catagory 1 size = \"+str(np.sum(TwoCatX_T[0][1]==1))+\", Catagory 0 size = \"+str(np.sum(TwoCatX_T[0][1]==0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Catagory 1 size = 105, Catagory 0 size = 126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fftGS1VMrOLU"
      },
      "source": [
        "With this new Catagory System, the distrabution is evenly split between the two catagories allowing for a better model to be trained. Once I had the new intial data set the same procceses that were preformed on the 8 catagory were done to the 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQGq0KCxd9Jr"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-94zZUcCw1Cv"
      },
      "source": [
        "### Full Feature Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfSbPvWyw9wv",
        "outputId": "a41fa895-3313-4dac-b984-fb1574756ff2"
      },
      "source": [
        "  i=0\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"All Catagories\")\r\n",
        "  print(bestAllNets[i])\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"Two Catagories\")\r\n",
        "  print(bestTwoNets[i])\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(6, [10, 100], 7, 'tanh') trained for 5000 epochs, final training error 0.9910\n",
            "Test\n",
            "Test percent correct 50.00\n",
            "      1          2           3          4          5    6\n",
            "1  50.0   0.000000    0.000000   0.000000  50.000000  0.0\n",
            "2   0.0  42.857143   14.285714  14.285714  28.571429  0.0\n",
            "3   0.0   0.000000   60.000000   0.000000  40.000000  0.0\n",
            "4   0.0   0.000000   20.000000  40.000000  40.000000  0.0\n",
            "5   0.0  16.666667    0.000000   0.000000  66.666667  0.0\n",
            "6   0.0   0.000000  100.000000   0.000000   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 50.00\n",
            "           2          3     4          5         6\n",
            "2  60.000000   0.000000  20.0  20.000000  0.000000\n",
            "3   0.000000  60.000000   0.0  40.000000  0.000000\n",
            "4  25.000000   0.000000  50.0  25.000000  0.000000\n",
            "5  18.181818   9.090909   0.0  54.545455  9.090909\n",
            "6   0.000000  33.333333   0.0  66.666667  0.000000\n",
            "--------------------------------------------------\n",
            "Two Catagories\n",
            "NeuralNetwork(6, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.9792\n",
            "Test\n",
            "Test percent correct 67.86\n",
            "           0          1\n",
            "0  75.000000  25.000000\n",
            "1  41.666667  58.333333\n",
            "\n",
            "Validate\n",
            "Test percent correct 75.00\n",
            "           0          1\n",
            "0  80.000000  20.000000\n",
            "1  27.777778  72.222222\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrO1Kr2txzO0"
      },
      "source": [
        "With the entire feature set, we are getting quite high accuracies for both the 2 and 8 catagory. For 8 catagory we can see that for this model the network mostly guesses the 5th catagory which is to be expected with our data set being skewed. For the extreams(0-1,6-7) the model is almost never correct which is also to be expected with our data. The 2 catagory model has extrmely high accuracy for the scope of the problem, but from all of my runs never breaks 80%. This could be do to many factors, i belive this is mainly caused by two factors. Firstly because of the 0% spliting point for the catagories there is no highly defined edges for the catagories. Because of this I could see a 1% increase could easily be confused with a decrease vice versa. Secondly I think that we are not taking acount of many more factors that effect the stock like how the stock change priviously can effect how it will change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3hxte2faGUl"
      },
      "source": [
        "### No Dow Jones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of2QG2CbZs9A",
        "outputId": "275247c6-381f-47de-9bcd-1af6a8b5fbe2"
      },
      "source": [
        "  i=1\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"All Catagories\")\r\n",
        "  print(bestAllNets[i])\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"Two Catagories\")\r\n",
        "  print(bestTwoNets[i])\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(5, [20, 50, 100, 50, 20], 7, 'tanh') trained for 5000 epochs, final training error 0.8095\n",
            "Test\n",
            "Test percent correct 41.38\n",
            "     1           2          3    4           5    6\n",
            "1  0.0  100.000000   0.000000  0.0    0.000000  0.0\n",
            "2  0.0   85.714286   0.000000  0.0   14.285714  0.0\n",
            "3  0.0    0.000000  20.000000  0.0   80.000000  0.0\n",
            "4  0.0   33.333333  16.666667  0.0   50.000000  0.0\n",
            "5  0.0   37.500000   0.000000  0.0   62.500000  0.0\n",
            "6  0.0    0.000000   0.000000  0.0  100.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 41.38\n",
            "     1          2     3    4           5    6\n",
            "1  0.0   0.000000   0.0  0.0  100.000000  0.0\n",
            "2  0.0  50.000000   0.0  0.0   50.000000  0.0\n",
            "3  0.0  25.000000  25.0  0.0   50.000000  0.0\n",
            "4  0.0  66.666667   0.0  0.0   33.333333  0.0\n",
            "5  0.0  42.857143   0.0  0.0   57.142857  0.0\n",
            "6  0.0   0.000000   0.0  0.0  100.000000  0.0\n",
            "--------------------------------------------------\n",
            "Two Catagories\n",
            "NeuralNetwork(5, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.8702\n",
            "Test\n",
            "Test percent correct 51.72\n",
            "           0          1\n",
            "0  53.846154  46.153846\n",
            "1  50.000000  50.000000\n",
            "\n",
            "Validate\n",
            "Test percent correct 58.62\n",
            "           0          1\n",
            "0  54.545455  45.454545\n",
            "1  38.888889  61.111111\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ui-M0ZOd9Jr"
      },
      "source": [
        "Once the Dow Jones data was removed we can see a clear drop in the accuracy of our models. The 8 catagory network changed to only guess 2 or 5 and at about a 50/50 split between the 2, to me this shows that its just guessing between the two high most common points in our data. For the 2 catagory its also 50/50, from this we can see that using the Dow jones was a highly influential feature and did help us represent how the overall market preformed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKdfobN2eASz"
      },
      "source": [
        "### Only Positive or Negitive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nr-LJSUgXFh",
        "outputId": "32d61758-6a92-431b-8285-c07e506fbd05"
      },
      "source": [
        "  i=2\r\n",
        "  print('|Only Positive|')\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"All Catagories\")\r\n",
        "  print(bestAllNets[i])\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"Two Catagories\")\r\n",
        "  print(bestTwoNets[i])\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  i=3\r\n",
        "  print('|Only Negitive|')\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"All Catagories\")\r\n",
        "  print(bestAllNets[i])\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestAllNets[i].use(AllCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,AllCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")\r\n",
        "  print(\"Two Catagories\")\r\n",
        "  print(bestTwoNets[i])\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][2])\r\n",
        "  print(\"Test\")\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][3]))\r\n",
        "  print()\r\n",
        "  print(\"Validate\")\r\n",
        "  Yclasses, probs=bestTwoNets[i].use(TwoCatX_T[i][4])\r\n",
        "  print(confusion_matrix(Yclasses,TwoCatX_T[i][5]))\r\n",
        "  print(\"--------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|Only Positive|\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(4, [100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8720\n",
            "Test\n",
            "Test percent correct 42.86\n",
            "      1          2          3           4          5    6\n",
            "1  50.0   0.000000   0.000000   25.000000  25.000000  0.0\n",
            "2   0.0  42.857143   0.000000   14.285714  42.857143  0.0\n",
            "3   0.0  40.000000  40.000000   20.000000   0.000000  0.0\n",
            "4   0.0   0.000000  20.000000   40.000000  40.000000  0.0\n",
            "5   0.0   0.000000  16.666667   16.666667  50.000000  0.0\n",
            "6   0.0   0.000000   0.000000  100.000000   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 42.86\n",
            "           2          3          4          5    6\n",
            "2  40.000000  60.000000   0.000000   0.000000  0.0\n",
            "3   0.000000  40.000000  40.000000  20.000000  0.0\n",
            "4  25.000000   0.000000  75.000000   0.000000  0.0\n",
            "5   9.090909   9.090909  36.363636  45.454545  0.0\n",
            "6   0.000000   0.000000  33.333333  66.666667  0.0\n",
            "--------------------------------------------------\n",
            "Two Catagories\n",
            "NeuralNetwork(4, [10, 20, 10], 2, 'tanh') trained for 5000 epochs, final training error 0.8710\n",
            "Test\n",
            "Test percent correct 60.71\n",
            "           0          1\n",
            "0  56.250000  43.750000\n",
            "1  33.333333  66.666667\n",
            "\n",
            "Validate\n",
            "Test percent correct 71.43\n",
            "           0          1\n",
            "0  50.000000  50.000000\n",
            "1  16.666667  83.333333\n",
            "--------------------------------------------------\n",
            "|Only Negitive|\n",
            "--------------------------------------------------\n",
            "All Catagories\n",
            "NeuralNetwork(4, [10, 100, 10], 7, 'tanh') trained for 5000 epochs, final training error 0.8617\n",
            "Test\n",
            "Test percent correct 35.71\n",
            "      1          2           3    4          5    6\n",
            "1  50.0   0.000000    0.000000  0.0  50.000000  0.0\n",
            "2   0.0  57.142857    0.000000  0.0  42.857143  0.0\n",
            "3   0.0  20.000000   20.000000  0.0  60.000000  0.0\n",
            "4   0.0   0.000000   60.000000  0.0  40.000000  0.0\n",
            "5   0.0  16.666667   16.666667  0.0  50.000000  0.0\n",
            "6   0.0   0.000000  100.000000  0.0   0.000000  0.0\n",
            "\n",
            "Validate\n",
            "Test percent correct 46.43\n",
            "           2          3    4          5    6\n",
            "2  40.000000  40.000000  0.0  20.000000  0.0\n",
            "3   0.000000  60.000000  0.0  40.000000  0.0\n",
            "4  50.000000  25.000000  0.0  25.000000  0.0\n",
            "5  18.181818   9.090909  0.0  72.727273  0.0\n",
            "6   0.000000  33.333333  0.0  66.666667  0.0\n",
            "--------------------------------------------------\n",
            "Two Catagories\n",
            "NeuralNetwork(4, [10, 100], 2, 'tanh') trained for 5000 epochs, final training error 0.9750\n",
            "Test\n",
            "Test percent correct 71.43\n",
            "           0          1\n",
            "0  75.000000  25.000000\n",
            "1  33.333333  66.666667\n",
            "\n",
            "Validate\n",
            "Test percent correct 53.57\n",
            "           0          1\n",
            "0  40.000000  60.000000\n",
            "1  38.888889  61.111111\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mf135lXgQZa"
      },
      "source": [
        "In comparing both feature sets to the origonal set we can see some small differences. For the only positive feature set it close in accuarcy to the origonal data set, but interestingly it is slightly more accurate in picking the the negitive change. For the only negitive only set is less accurate than the both positive only and the origonal feature sets on average, but clearly adds value in some cases. This is shown by the Test accuracy being higher than in the positive only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppPCp4rcd9Jr"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eOoFJtdd9Js"
      },
      "source": [
        "From my result it is clear that using sentment analysis infomation on articles and tweets do help improve models made on tesla stock. This is due to the fact that they can help give a metric on public opinion to the model. It is also clear from the NO Dow Jones models that this public opinon information is far from enough to adiquitly model the stocks alone, but is more of a useful addition. From the results of the only positive or negitive model we can see that the positive data is slightly more useful in modeling; but by using both you are able to get a more accurate and more general model.\r\n",
        "\r\n",
        "To improve this model there are many steps that could be taken.A major improvement would be, a more complete data set to train on. This data set was trimed down to less than a 20th of the origonal data, due to missing values.   Another area of improvement would be a more comprehensive feature list. By adding information on the stocks previous states we could more accuratly predict the future stock, because of the stocks chaotic nature. And finaly a major enhancement would be a more optimal selection of catagorys. This helps the network make clearer distinctions between each catagory when traning.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsJlG37Gd9Js"
      },
      "source": [
        "### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FGqr3avd9Js"
      },
      "source": [
        "[1]Apache, “apache/hadoop.” [Online]. Available: https://github.com/apache/hadoop.\r\n",
        "\r\n",
        "[2]D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” CoRR, 2015.\r\n",
        "\r\n",
        "[3]JohnSnowLabs, “JohnSnowLabs/spark-nlp,” GitHub. [Online]. Available: https://github.com/JohnSnowLabs/spark-nlp.\r\n",
        "\r\n",
        "[4]Matplotlib, “matplotlib/matplotlib,” GitHub. [Online]. Available: https://github.com/matplotlib/matplotlib.\r\n",
        "\r\n",
        "[5]Pytorch, “pytorch/pytorch,” GitHub. [Online]. Available: https://github.com/pytorch/pytorch.\r\n",
        "\r\n",
        "[6]\"twintproject/twint\", GitHub, 2020. [Online]. Available: https://github.com/twintproject/twint. [Accessed: 03- Dec- 2020].\r\n",
        "\r\n",
        "[7]\"NumPy\", Numpy.org, 2020. [Online]. Available: https://numpy.org/. [Accessed: 03- Dec- 2020].\r\n",
        "\r\n",
        "[8]\"Apache Spark™ - Unified Analytics Engine for Big Data\", Spark.apache.org, 2020. [Online]. Available: https://spark.apache.org/. [Accessed: 03- Dec- 2020].\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "XMBrJrI9d9Jt"
      },
      "source": [
        "Your report for a single person team should contain approximately 2,000 words times number of team members, in markdown cells.  You can count words by running the following python code in your report directory.  Projects with two people, for example, should contain 4,000 to 8,000 words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIaph46l2gIu"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA22tBY42p-R"
      },
      "source": [
        "https://www.kaggle.com/timoboz/tesla-stock-data-from-2010-to-2020\r\n",
        "\tContains all tesla opening and closing stock prices from 2010 to 2020 by day.\r\n",
        "\r\n",
        "https://www.kaggle.com/snapcrack/all-the-news?select=articles1.csv\r\n",
        "\tData set of 1.2gb of news articles from multiple publications between 2015-2017.\r\n",
        "\r\n",
        "https://www.kaggle.com/rootuser/worldnews-on-reddit?select=reddit_worldnews_start_to_2016-11-22.csv\r\n",
        "Data set of WorldNews on Reddit from 2008 to 2016\r\n",
        "\r\n",
        "https://www.kaggle.com/rmisra/news-category-dataset\r\n",
        "Data set of HuffPost articles from 2012 to 2018. \r\n",
        "\r\n",
        "https://www.kaggle.com/aaron7sun/stocknews\r\n",
        "Data set of Reddit new ranked by user upvotes and a data set of the Dow Jones Industrial Average from 2008 to 2016.\r\n",
        "\r\n",
        "https://drive.google.com/drive/folders/1ZS9fMyR4vruguWFcq_B6mwzp_dtttn24?usp=sharing\r\n",
        "Dataset containing every tweet from 2011/1/1 - 2020/12/1 that contains either “tesla” or “@elonmusk”\r\n",
        "\t\r\n",
        "https://finance.yahoo.com/quote/%5EDJI/history?period1=1262476800&period2=1606953600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=false.\r\n",
        "Dataset of historical stock values of Dow Jones Industrial Average (DJI) through the dates:Jan 02, 2010 - Dec 02, 2020\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU5eKQUDd9Jt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "f742c6e9-e097-4001-82ab-d22adfc8e21f"
      },
      "source": [
        "import io\n",
        "from nbformat import current\n",
        "import glob\n",
        "nbfile = glob.glob('Project Report Example.ipynb')\n",
        "if len(nbfile) > 1:\n",
        "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
        "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
        "    nb = current.read(f, 'json')\n",
        "word_count = 0\n",
        "for cell in nb.worksheets[0].cells:\n",
        "    if cell.cell_type == \"markdown\":\n",
        "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
        "print('Word count for file', nbfile[0], 'is', word_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-3813ea17e362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'More than one ipynb file. Using the first one.  nbfile='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}